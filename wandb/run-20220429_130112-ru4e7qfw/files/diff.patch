diff --git a/Model/dataset/preproccessing.py b/Model/dataset/preproccessing.py
index bb33140..4b31994 100644
--- a/Model/dataset/preproccessing.py
+++ b/Model/dataset/preproccessing.py
@@ -4,11 +4,11 @@ from Model import *
 class PreProccessing:
     def __init__(
         self,
-        random_vertical_flip: bool = True,
-        color_jitter: bool = True,
-        random_grayscale: bool = True,
-        random_horizontal_flip: bool = True,
-        random_rotation: bool = True,
+        random_vertical_flip: bool = False,
+        color_jitter: bool = False,
+        random_grayscale: bool = False,
+        random_horizontal_flip: bool = False,
+        random_rotation: bool = False,
     ) -> None:
         self.color_jitter = color_jitter
         self.random_grayscale = random_grayscale
@@ -43,8 +43,11 @@ class PreProccessing:
             self.random_rotation_pp()
         if self.random_vertical_flip:
             self.random_vertical_flip_pp()
-        transformation = Compose(self.compose_list)
-        img = np.array(transformation(Image.fromarray(img)))
+        try:
+            transformation = Compose(self.compose_list)
+            img = np.array(transformation(Image.fromarray(img)))
+        except:
+            pass
         img = img / 255.0
         return img
 
diff --git a/Model/help_funcs.py b/Model/help_funcs.py
index 0ea06fa..50f11b2 100644
--- a/Model/help_funcs.py
+++ b/Model/help_funcs.py
@@ -5,6 +5,7 @@ from Model.metrics import *
 
 class Help_Funcs:
     def train(
+        self,
         PROJECT_NAME,
         name,
         epochs,
@@ -19,16 +20,21 @@ class Help_Funcs:
         optimizer,
     ):
         m = Metrics()
-        wandb.init(project=PROJECT_NAME, name=name)
+        wandb.init(
+            project=PROJECT_NAME,
+            name=name,
+            config={"device": device, "batch_size": batch_size, "epochs": epochs},
+        )
+        wandb.watch(model)
         for _ in tqdm(range(epochs)):
             for idx in range(0, len(X_train), batch_size):
-                X_batch = X_train[idx : idx + batch_size].to(device)
-                y_batch = y_train[idx : idx + batch_size].to(device)
+                X_batch = X_train[idx : idx + batch_size].float().to(device)
+                y_batch = y_train[idx : idx + batch_size].float().to(device)
                 preds = model(X_batch)
-                loss = criterion(preds)
-                optimizer.step()
-                loss.backward()
+                loss = criterion(preds, y_batch)
                 optimizer.zero_grad()
+                loss.backward()
+                optimizer.step()
             wandb.log(
                 {
                     "Accuracy Batch": m.accuracy(model, X_batch, y_batch),
@@ -37,6 +43,7 @@ class Help_Funcs:
                     "Loss": m.loss(model, X_test, y_test, criterion),
                 }
             )
+        wandb.watch(model)
         wandb.finish()
         return (
             PROJECT_NAME,
diff --git a/Model/modelling/pytorch_imp.py b/Model/modelling/pytorch_imp.py
index a5ffcc4..49a9ec8 100644
--- a/Model/modelling/pytorch_imp.py
+++ b/Model/modelling/pytorch_imp.py
@@ -33,8 +33,8 @@ class CNN(Module):
         self.conv2batchnorm = BatchNorm2d(16)
         self.conv3 = Conv2d(16, 32, (3, 3))
         self.conv4batchnorm = BatchNorm2d(32)
-        self.conv5 = Conv2d(32, 64, (3, 3))
-        self.linear1 = Linear(64 * 3 * 3, 128)
+        self.conv5 = Conv2d(32, 64, (1, 1))
+        self.linear1 = Linear(64 * 1 * 1, 128)
         self.linear2batchnorm = BatchNorm1d(128)
         self.linear3 = Linear(128, 256)
         self.linear4batchnorm = Linear(256, 512)
@@ -48,8 +48,7 @@ class CNN(Module):
         preds = self.max_pool2d(self.activation(self.conv3(preds)))
         preds = self.max_pool2d(self.activation(self.conv4batchnorm(preds)))
         preds = self.conv5(preds)
-        print(preds.shape)
-        preds = preds.view(-1, 64 * 3 * 3)
+        preds = preds.view(-1, 64 * 1 * 1)
         preds = self.activation(self.linear1(preds))
         preds = self.activation(self.linear2batchnorm(preds))
         preds = self.activation(self.linear3(preds))
diff --git a/run.py b/run.py
index 3a0b31c..c64d834 100644
--- a/run.py
+++ b/run.py
@@ -6,19 +6,20 @@ X, y, classes, labels, idx, labels_r, X_train, y_train, X_test, y_test = ds.load
 print(len(X_train), len(X_test), len(y_train), len(y_test))
 print("Loaded Data")
 print("Creating Model")
-model = CNN().to(DEVICE)
+model = CNN(idx_of_classes=idx).to(DEVICE)
 criterion = MSELoss()
-optimizer = Adam(model.parameter(), lr=0.001)
+optimizer = Adam(model.parameters(), lr=0.001)
 print("Created Model")
 print("Training Model")
 hp = Help_Funcs()
 hp.train(
-    "BaseLine",
+    PROJECT_NAME,
+    "BaseLine CNN",
     EPOCHS,
-    X_train,
-    y_train,
-    X_test,
-    y_test,
+    X_train.view(-1, 1, 28, 28).to(DEVICE),
+    y_train.to(DEVICE),
+    X_test.view(-1, 1, 28, 28),
+    y_test.to(DEVICE),
     BATCH_SIZE,
     DEVICE,
     model,
