diff --git a/Model/dataset/preproccessing.py b/Model/dataset/preproccessing.py
index bb33140..4b31994 100644
--- a/Model/dataset/preproccessing.py
+++ b/Model/dataset/preproccessing.py
@@ -4,11 +4,11 @@ from Model import *
 class PreProccessing:
     def __init__(
         self,
-        random_vertical_flip: bool = True,
-        color_jitter: bool = True,
-        random_grayscale: bool = True,
-        random_horizontal_flip: bool = True,
-        random_rotation: bool = True,
+        random_vertical_flip: bool = False,
+        color_jitter: bool = False,
+        random_grayscale: bool = False,
+        random_horizontal_flip: bool = False,
+        random_rotation: bool = False,
     ) -> None:
         self.color_jitter = color_jitter
         self.random_grayscale = random_grayscale
@@ -43,8 +43,11 @@ class PreProccessing:
             self.random_rotation_pp()
         if self.random_vertical_flip:
             self.random_vertical_flip_pp()
-        transformation = Compose(self.compose_list)
-        img = np.array(transformation(Image.fromarray(img)))
+        try:
+            transformation = Compose(self.compose_list)
+            img = np.array(transformation(Image.fromarray(img)))
+        except:
+            pass
         img = img / 255.0
         return img
 
diff --git a/Model/help_funcs.py b/Model/help_funcs.py
index 0ea06fa..3426b6a 100644
--- a/Model/help_funcs.py
+++ b/Model/help_funcs.py
@@ -5,6 +5,7 @@ from Model.metrics import *
 
 class Help_Funcs:
     def train(
+        self,
         PROJECT_NAME,
         name,
         epochs,
@@ -19,13 +20,18 @@ class Help_Funcs:
         optimizer,
     ):
         m = Metrics()
-        wandb.init(project=PROJECT_NAME, name=name)
+        wandb.init(
+            project=PROJECT_NAME,
+            name=name,
+            config={"device": device, "batch_size": batch_size, "epochs": epochs},
+        )
+        wandb.watch(model)
         for _ in tqdm(range(epochs)):
             for idx in range(0, len(X_train), batch_size):
-                X_batch = X_train[idx : idx + batch_size].to(device)
-                y_batch = y_train[idx : idx + batch_size].to(device)
+                X_batch = X_train[idx : idx + batch_size].float().to(device)
+                y_batch = y_train[idx : idx + batch_size].float().to(device)
                 preds = model(X_batch)
-                loss = criterion(preds)
+                loss = criterion(preds, y_batch)
                 optimizer.step()
                 loss.backward()
                 optimizer.zero_grad()
@@ -37,6 +43,7 @@ class Help_Funcs:
                     "Loss": m.loss(model, X_test, y_test, criterion),
                 }
             )
+        wandb.watch(model)
         wandb.finish()
         return (
             PROJECT_NAME,
diff --git a/Model/modelling/pytorch_imp.py b/Model/modelling/pytorch_imp.py
index a5ffcc4..cbf6f7c 100644
--- a/Model/modelling/pytorch_imp.py
+++ b/Model/modelling/pytorch_imp.py
@@ -33,7 +33,7 @@ class CNN(Module):
         self.conv2batchnorm = BatchNorm2d(16)
         self.conv3 = Conv2d(16, 32, (3, 3))
         self.conv4batchnorm = BatchNorm2d(32)
-        self.conv5 = Conv2d(32, 64, (3, 3))
+        self.conv5 = Conv2d(32, 64, (1, 1))
         self.linear1 = Linear(64 * 3 * 3, 128)
         self.linear2batchnorm = BatchNorm1d(128)
         self.linear3 = Linear(128, 256)
diff --git a/run.py b/run.py
index 3a0b31c..9ebc270 100644
--- a/run.py
+++ b/run.py
@@ -8,16 +8,17 @@ print("Loaded Data")
 print("Creating Model")
 model = CNN().to(DEVICE)
 criterion = MSELoss()
-optimizer = Adam(model.parameter(), lr=0.001)
+optimizer = Adam(model.parameters(), lr=0.001)
 print("Created Model")
 print("Training Model")
 hp = Help_Funcs()
 hp.train(
+    PROJECT_NAME,
     "BaseLine",
     EPOCHS,
-    X_train,
+    X_train.view(-1, 1, 28, 28),
     y_train,
-    X_test,
+    X_test.view(-1, 1, 28, 28),
     y_test,
     BATCH_SIZE,
     DEVICE,
